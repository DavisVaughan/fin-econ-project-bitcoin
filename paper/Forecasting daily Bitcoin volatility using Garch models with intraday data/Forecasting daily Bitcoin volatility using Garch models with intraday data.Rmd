---
title: Forecasting daily Bitcoin volatility using GARCH models and intraday data

# Use letters for affiliations
author:
  - name: Davis Vaughan
    affiliation: University of North Carolina at Charlotte

# For footer text  TODO(fold into template, allow free form two-authors)
lead_author_surname: Vaughan

# Place DOI URL or CRAN Package URL here

# Abstract
abstract: |
  GARCH modeling has been a topic of interest for many years, with much
  discussion around the forecasting performance of the models in the context of equities, but relatively 
  few papers have applied the technique to crytocurrencies. The rapid 
  growth of Bitcoin over the past few years, coupled with its highly volatile
  price, makes it a perfect candidate for analysis. This paper produces daily
  forecasts of variance through an aggregation of intraday forecasts, and compares
  the performance against measures of realized variance. Aggregation methods are
  found to be more accurate than 1-step ahead daily forecasts.

# Optional: Acknowledgements
acknowledgements: |
  This work would not have been possible without the excellent 
  [rugarch](https://cran.r-project.org/web/packages/rugarch/rugarch.pdf) R package.
  The paper formatting was handled completely by the [pinp](https://cran.r-project.org/web/packages/pinp/index.html)
  R package. Additionally, the [tidyverse](https://www.tidyverse.org/) packages from RStudio were invaluable for data
  manipulation.

# Optional: One or more keywords
keywords:
  - GARCH
  - intraday
  - Bitcoin

# Font size of the document, values of 9pt (default), 10pt, 11pt and 12pt
fontsize: 10pt

# Optional: Force one-column layout, default is two-column
one_column: true

# Optional: Enables lineno mode, but only if one_column mode is also true
#lineno: true

# Optional: Enable one-sided layout, default is two-sided
#one_sided: true

# Optional: Enable section numbering, default is unnumbered
numbersections: true

# Optional: Specify the depth of section number, default is 5
#secnumdepth: 5

# Optional: Skip inserting final break between acknowledgements, default is false
skip_final_break: true

# Optional: Bibliography 
bibliography: pinp

# Optional: Enable a 'Draft' watermark on the document
watermark: false

# Customize footer, eg by referencing the vignette
footer_contents: "Vaughan"

# Produce a pinp document
output: pinp::pinp

header-includes:
   - \usepackage{tabularx,colortbl,multirow,hhline,mathtools}
   
# Required: Vignette metadata for inclusion in a package.
vignette: >
  %\VignetteIndexEntry{YourPackage-vignetteentry}
  %\VignetteKeywords{YourPackage, r, anotherkeyword}
  %\VignettePackage{YourPackage}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r, warning=FALSE, echo=FALSE, message=FALSE, cache = TRUE}
# Packages used
library(dplyr)
library(ggplot2)
library(huxtable)
library(tibble)
library(tidyr)
library(purrr)
```


# Introduction 

Forecasting daily volatility accurately is an important input into many 
applications in the financial industry. From a risk management perspective,
Value at Risk (VaR) relies heavily on having an accurate measure of volatility. 
In fact, for the simple variance covariance method of estimating Value at Risk
using a Normal distribution, the 95% estimate is simply $-1.65 \sigma$. More
generally, understanding future volatility allows a firm to hedge appropriately,
and strategically place themselves for the times ahead.

Since the creation of GARCH modeling by Bollershev, \cite{Garch}, there has been an explosion of literature extending the GARCH model to be more flexible, to include external
regressors, and to attempt to capture asymmetric effects of returns. One of the 
most interesting papers, co-authored by Bollerslev himself, attempts to combat
the argument that, while GARCH models often seem to fit well in-sample, they have
poor forecasting performance, \cite{AndersonBollerslev}. He argues that the 
ex-post measure of variance commonly being used, the daily squared return,
is a poor estimate of the true variance that one should measure performance 
against. Rather, one should use a summation of squared intraday returns 
aggregated to the daily level, termed _realized variance_, as a measure of that day's variance. Such an approach is one of the focuses of this paper.

The second focus is on an extension of Ñíguez's approach of 
forecasting monthly volatility of the EuroStoxx 50, \cite{MonthlyVol}. There, he fits a GARCH model to daily data, forecasts the next month's worth
of daily volatility, rolls the model forward one month, and repeats the process.
The daily volatility forecasts for each month are then squared and summed to form a forecast of the next month's variance, similar in spirit to the concept of realized variance.

In this paper, Bitcoin volatility is forecasted using GARCH(1,1) models under the
Normal and Student-t distributions. Models are fit using 5-minute data, and daily
forecasts are created from aggregated 5-minute forecasts. These forecasts are
compared against those generated from models fit to daily data. 
Model performance is evaluated using mean squared error (MSE), mean absolute percent error (MAPE), and $R^2$ coefficients from Minzer-Zarnowitz (MZ) regressions.

# Volatility modeling

Let $r_{t_i} = log(P_{t_i}) - log(P_{t_{i-1}})$ 
represent the i-th 5-minute log return. The index `t` will refer to 5-minute sampling.
Additionally, the j-th daily log return is denoted as $r_{d_j} = log(P_{d_j}) - log(P_{d_{j-1}})$.
Because Bitcoin is continuously traded 24/7, there are 288 5-minute samples per
day.
 
The data set used contains 5-minute prices for the Bitstamp exchange over the period
from 2016-01-01 to 2017-10-20 gathered from the Kaggle competition, 
Bitcoin Historical Data. A link to the competition is provided in the references under
\cite{Kaggle}. Data after 2017-06-30 23:55:00 are considered out of sample.
Figure 1 displays some descriptive plots of the 5-minute data
along with the daily data. Notably, Bitcoin, like most other securities,
exhibits the fact that volatility is not constant throughout time.

\begin{figure*}
  \begin{center}
    \includegraphics[width=1.00\textwidth, height=8.5in]{../../visualizations/returns} 
  \end{center}
  \caption{Descriptive plots of 5-minute and daily returns}\label{fig}
\end{figure*}

Volatility is modeled using GARCH(1,1) models, fitted at both the 5-minute and
daily levels. Two distributions are used for returns, Normal and Student-t. For 
simplicity, the mean of the return process is set to 0. Additionally, after finding
it insignificant, the constant in the GARCH model is set to 0. The resulting
model for 5-minute sampling is,

\begin{equation}
  \begin{aligned}
r_{t_i}         &= \epsilon_{t_i} \\
\epsilon_{t_i}  &= \sigma_{t_i}  z_{t_i} \\
\sigma_{t_i}    &= \alpha \epsilon_{t_{i-1}}  + \beta \sigma_{t_{i-1}}
       \label{eqn:example} 
  \end{aligned}
\end{equation}

with $z_{t_i}$ represented as either $z_{t_i} \sim N(0,1)$ or $z_{t_i} \sim t_{\nu}$ 
depending on the underlying distribution. To differentiate the two distributions,
GARCH(1,1)-N and GARCH(1,1)-T will denote GARCH(1,1) under the Normal and Student-t 
distributions respectively. The daily model is defined similarly, with $t_i$ replaced
by $d_j$.

# In sample estimation results

The results of the GARCH(1,1) in sample estimations are reported below in Table 1.
Consistent with literature, $\alpha$ and $\beta$ estimates sum to a value close
to 1, demonstrating the highly persistent nature of volatility at both the 5-minute
and daily levels. Both sampling intervals produce a degrees of freedom estimate 
near 4, resulting from the heavy tailed nature of the returns. Not shown here 
are additional fits from GJR-GARCH models that attempted to capture any 
asymmetric properties of the distribution. Interestingly, none of the asymmetric
parameters were statistically significant, suggesting that Bitcoin does not 
react more strongly to negative movements.

```{r , results='asis', echo=FALSE, cache = TRUE}
fit <- readr::read_rds("../../models/garch-model-5-min.rds")
fit_daily <- readr::read_rds("../../models/garch-model-daily.rds")

prepare_coef_norm <- function(matcoef) {
  matcoef %>%
    as_tibble() %>%
    slice(-1) %>%
    add_column(Coefficient = c("$\\alpha$", "$\\beta$"), .before = 1) %>%
    select(-` t value`)
}

prepare_coef_student_t <- function(matcoef) {
  matcoef %>%
    as_tibble() %>%
    slice(-1) %>%
    add_column(Coefficient = c("$\\alpha$", "$\\beta$", "$\\nu$"), .before = 1) %>%
    select(-` t value`)
}

df_coef <- tibble::tibble(
  f = c(prepare_coef_norm, prepare_coef_student_t, 
        prepare_coef_norm, prepare_coef_student_t),
  
  params = list( list(fit$fit[[1]]@fit$robust.matcoef),
                 list(fit$fit[[2]]@fit$robust.matcoef),
                 list(fit_daily$fit[[1]]@fit$robust.matcoef),
                 list(fit_daily$fit[[2]]@fit$robust.matcoef))
)

all_coef <- purrr::invoke_map(df_coef$f, .x = df_coef$params) %>%
  bind_rows() %>%
  select(everything(), 
         `Robust Std Error` = ` Std. Error`,
         `P-value` = `Pr(>|t|)`)
  
triple_blank <- function() {
  c("", "", "")
}

ht <- huxtable(all_coef) %>%
  add_colnames() %>%
  set_escape_contents(everywhere, 1, FALSE) %>%
  insert_row("Normal: 5-min",    triple_blank(), after = 1) %>%
  insert_row("Student-t: 5-min", triple_blank(), after = 4) %>%
  insert_row("Normal: daily",    triple_blank(), after = 8) %>%
  insert_row("Student-t: daily", triple_blank(), after = 11) %>%
  set_number_format(everywhere, everywhere, "%5.3f") %>%
  set_bold(matrix(c(2,5,9,12)), 1, TRUE) %>%
  set_bold(1, everywhere, TRUE) %>%
  set_align(everywhere, everywhere, 'center') %>%
  set_align(everywhere, 1, 'right') %>%
  set_bottom_border(matrix(c(1, 4, 8, 11)), everywhere, value = .3) %>%
  set_right_border(everywhere, 1, .3) %>%
  set_left_padding(everywhere, 1, -30) %>%
  set_caption("Estimation results of GARCH(1,1) in sample fits under different distributions and at different sampling intervals.")

ht[1,1] <- ""

ht
```

# Volatility forecasting and realized variance

The forecasting methods used can be broken down into two sections: the procedure
used for including new information in the forecasts, and the performance measure
used to validate against.

## Forecasting methodology

For daily sampling, the forecasting procedure is as follows:

1) An initial GARCH(1,1) model is fit using the first 546 daily returns.
2) A 1-step ahead forecast of the next day's variance is generated, denoted $\hat{h}_{d_{j+1}}$.
3) Using a moving window, the oldest day's return is dropped, the newest 
day's return is included, and the model is refit.
4) Steps 2 and 3 are repeated to generate 111 daily forecasts of variance.

For 5-minute sampling, the forecasting procedure is slightly more complicated 
to avoid any look ahead bias. The procedure is:

1) An initial GARCH(1,1) model is fit using the first 157535 5-minute returns. 
This ends the fit on 2017-06-30 23:55:00, the end of that day.
2) The next day's worth of 5-minute variance forecasts are generated recursively. This 
results in 288 5-minute forecasts for the next day (24 hours x 60 minutes / 5 minutes).
3) Those 5-minute forecasts are summed to generate a forecast of variance for that day, 
denoted $\hat{H}_{d_{j+1}} = \sum_{i = 1}^{288} \hat{h}_{t_{i + 288(j+1)}}$. 
The notation of $288(j+1)$ is used to ensure that the correct day's worth of 
5-minute variance forecasts are summed.
4) Using a moving window, the oldest 288 5-minute returns are dropped, the 
newest 288 5-minute returns are included, and the model is refit. This effectively 
shifts the model 1 day forward.
5) Steps 2-4 are repeated to generate 111 daily forecasts of variance.

Ideally, the aggregation approach allows the model to be more flexible, and take into
account a finer level of detail. These approaches were repeated for both
Normal and Student-t distributions. 

To generate n-step ahead forecasts under the GARCH(1,1) model, the following
recursive formula was used:

\begin{equation}
  \begin{aligned}
\hat{h}_{t+n} = (\alpha + \beta)^{n-1} \hat{h}_{t+1}
       \label{eqn:forecast} 
  \end{aligned}
\end{equation}

## Realized variance

Equally as important as the forecast methodology is the proxy of variance that one
measures performance against. Because volatility is unobservable, some estimate 
of the true volatility is required to calculate any kind of performance measure.
A common measure of forecasting performance for daily variance is to use that 
day's squared return, $r_{d_{j+1}}^2$. However, as noted by Bollershev, while this
is a consistent estimator of conditional variance, it is incredibly noisy, \cite{Garch}.
Bollershev proposes the use of intraday information to estimate the daily variance. 
The technique, termed _realized variance_, is adapted here as the sum of squared
returns for the 288 5-minute returns in each day. Formally:

$$ RV_{d_{j+1}} = \sum_{i = 1}^{288} r_{t_{i + 288(j+1)}}^2 $$

Both the daily squared return and the realized variance will be used to 
generate performance metrics for the models, and their uses as benchmarks
will be compared.

Figure 2 demonstrates the difference between the proxies of realized variance and
the squared daily return. Squared return is incredibly noisy in comparison to 
realized variance, especially in periods of higher volatility, where the estimate
can spike to unrealistically high amounts.

\begin{figure*}
  \begin{center}
    \includegraphics[width=1.00\textwidth]{../../visualizations/compare_proxies} 
  \end{center}
  \caption{Realized variance VS Squared return in the out of sample period. Squared return is very noisy, and not the best estimator to benchmark performance against.}\label{fig}
\end{figure*}

# Forecasting results

The plot in Figure 3 shows the out of sample forecasting performance of the GARCH(1,1)-N
models in comparision to the realized variance. The daily forecasts generated 
from aggregated 5-minute forecasts have more flexibility to adapt quickly to 
the movements in the level of variance compared to the GARCH(1,1)-N model 
fit to daily data, especially in transition periods from high to low volatility. 

The extreme forecast from the aggregation model resulting in variance above 0.04 in mid July
is of cause for concern, and discussion of this is addressed in the next section.

\begin{figure*}
  \begin{center}
    \includegraphics[width=1.00\textwidth]{../../visualizations/compare_normal_forecasts} 
  \end{center}
  \caption{Out of sample performance of GARCH(1,1) models under the Normal distribution. Daily forecasts of variance from aggregated 5-minute forecasts appear to be more flexible in adapting quickly to changes in the overall level of variance, especially in transition periods from high to low variance like mid September.}\label{fig}
\end{figure*}

Figure 4 compares the Normal and Student-t forecasts for the 5-minute aggregation
technique. Student-t forecasts tend to be higher in periods of high volatility,
and slightly lower in periods of lower volatility, but overall they follow a 
similar track.

\begin{figure*}
  \begin{center}
    \includegraphics[width=1.00\textwidth]{../../visualizations/compare_normal_vs_student_forecasts} 
  \end{center}
  \caption{Out of sample performance of Normal VS Student-t GARCH(1,1) models
  using the 5-minute aggregation technique. Student-t forecasts tend to swing
  more wildly, but overall there is not much difference.}\label{fig}
\end{figure*}

Table 2 displays a number of performance metrics from the GARCH(1,1) models
under the Normal distribution. For brevity, Student-t results are not included as they
are not very different from Normal. Included metrics are Mean Absolute Percent Error 
(MAPE), Root Mean Squared Error (RMSE), and the intercept, slope and $R^2$ from
Minzer-Zarnowitz (MZ) regression of the corresponding proxy on the forecasts. For
all metrics, the realized variance proxy implied the models forecasted
better than when compared against the squared daily return proxy. 

The first two columns correspond to the 5-minute aggregation method of daily forecasts, 
and the forecasts from daily models respectively. While the MAPE of the 5-minute
method is lower than for daily, the RMSE is much higher. This is a direct result
of the incredibly large forecast above 0.04 from the 5-minute method that can be seen in Figure 3. 
Column 3 contains the metrics calculated again for the 5-minute method, but with
the removal of that one large forecast.
After removing that single point, the RMSE drops
below the daily method, the MZ slope coefficient jumps up to a value much closer
to 1, and the MZ $R^2$ increases significantly. This finding prompted a further
analysis to attempt to understand the outlier, outlined in the next section.

```{r , results='asis', echo=FALSE, cache = TRUE}
all_table_data <- readRDS("../../paper-tables/performance_table_data.rds")

mse_mape <- all_table_data %>%
  filter(distribution == "Normal") %>%
  select(-distribution) %>%
  gather(key = "proxy", value = "variance_proxy", -date, -daily_variance_forecast, -forc_style) %>%
  group_by(forc_style, proxy) %>%
  summarise(mse  = mean((variance_proxy - daily_variance_forecast) ^ 2),
            rmse = sqrt(mse),
            mape = mean(abs(variance_proxy - daily_variance_forecast) / sqrt(variance_proxy))) %>%
  gather(key = "metric", value = "value", -forc_style, -proxy) %>%
  spread(forc_style, value) %>%
  filter(metric != "mse")

MZ <- all_table_data %>%
  filter(distribution == "Normal") %>%
  select(-distribution) %>%
  gather(key = "proxy", value = "variance_proxy", -date, -daily_variance_forecast, -forc_style) %>%
  group_by(forc_style, proxy) %>%
  nest() %>%
  mutate(lm_fit = map(data, ~lm(variance_proxy ~ daily_variance_forecast, data = .x)),
         tidy = map(lm_fit, broom::tidy),
         glance = map(lm_fit, broom::glance)) 

MZ_coef <- MZ %>%
  unnest(tidy, .drop = TRUE) %>%
  select(forc_style:std.error) %>%
  gather(key, value, -forc_style, -proxy, -term) %>%
  unite(col = "metric", term, key) %>%
  spread(forc_style, value)

MZ_r2 <- MZ %>%
  unnest(glance, .drop = TRUE) %>%
  select(forc_style:r.squared) %>%
  gather(metric, value, r.squared) %>%
  spread(forc_style, value)

hux_data <- bind_rows(mse_mape, MZ_coef, MZ_r2) %>%
  mutate(metric = recode(metric, "mape" = "MAPE", "rmse" = "RMSE", 
                         "(Intercept)_estimate" = "MZ Intercept", 
                         "(Intercept)_std.error" = "MZ Intercept Std. Error",
                         "daily_variance_forecast_estimate" = "MZ Slope",
                         "daily_variance_forecast_std.error" = "MZ Slope Std. Error",
                         "r.squared" = "MZ $R^2$")) %>%
  arrange(proxy) %>%
  select(metric, "5-Min" = "5min", "Daily" = "daily", 
         "5-Min No Outlier" = "5min_filtered")

  
triple_blank <- function() {
  c("", "", "")
}

hux_data <- hux_data %>%
  mutate_if(is.numeric, round, 5)

hux_data[with(hux_data, which(metric == "MZ Intercept Std. Error" | metric == "MZ Slope Std. Error")),] <- mutate_if(hux_data[with(hux_data, which(metric == "MZ Intercept Std. Error" | metric == "MZ Slope Std. Error")),],
                                                                                   is.double, 
                                                                                   funs(paste0("(", ., ")")))

hux_data[with(hux_data, which(metric == "MZ Intercept Std. Error" | metric == "MZ Slope Std. Error")),] <- mutate(hux_data[with(hux_data, which(metric == "MZ Intercept Std. Error" | metric == "MZ Slope Std. Error")),],
                                                                                metric = replace(metric, 1:4, NA))

ht <- hux_data %>%
  huxtable() %>%
  
  # Column names
  add_colnames() %>%
  
  # Proxy rows
  insert_row("Proxy: RV",    triple_blank(), after = 1) %>%
  insert_row("Proxy: $r^2$", triple_blank(), after = 9) %>%
  
  # Number rounding
  set_number_format(everywhere, everywhere, "%5.4f") %>%
  
  # Bold
  set_bold(matrix(c(2,10)), 1, TRUE) %>%
  set_bold(1, everywhere, TRUE) %>%
  
  # Alignment
  set_align(everywhere, everywhere, 'center') %>%
  set_align(everywhere, 1, 'right') %>%
  
  # Padding
  set_all_padding(value = 10) %>%
  set_top_padding(c(6, 8, 14, 16), everywhere, 0) %>%
  set_bottom_padding(c(5, 7, 13, 15), everywhere, 0) %>%
  set_left_padding(everywhere, 1, -40) %>%

  # Borders
  set_bottom_border(matrix(c(1, 9)), everywhere, value = .3) %>%
  set_right_border(everywhere, 1, .3) %>%
  
  # Escape latex
  set_escape_contents(everywhere, 1, FALSE) %>%
  set_escape_contents(1, everywhere, FALSE) %>%
  
  set_caption("Out of sample performance of GARCH(1,1) Normal models. MAPE for 5-min is lower than for daily. Across the board, using RV as a proxy over $r^2$ gives more accurate results. Removing the 1 extreme forecast from the 5-min method results in a much higher MZ $R^2$, and a MZ slope much closer to 1.")


ht[1,1] <- ""

ht
```

# Applying a filter

The large variance forecast from the 5-minute model on 2017-07-21 results from
a highly volatile period immediately before the end of the day on 2017-07-20.
The 5-minute returns near the end of that day were highly out of the ordinary, 
which resulted in forecasts in the 5-minute model that were very high. The high 
persistance of the model predicted a very slow decay in the variance over the
next day, resulting in the aggregate forecast of variance for that day that was
much larger than the realized variance.  

As a potential remedy to this, a filter was applied to the returns to remove outliers before
running the GARCH models and making the forecasts. Specifically, values outside 5 standard deviations
of the mean were replaced with the mean. Figure 5 displays the results of the 
forecasts before and after the filtration. The filtration "fixed" the large 
variance forecast, and otherwise the forecasts look very similar. Table 3 displays
the additional performance metrics for the filtered Normal model. The performance
is similar to the results from Table 2 with the 1 forecast outlier removed.

\begin{figure*}
  \begin{center}
    \includegraphics[width=1.00\textwidth]{../../visualizations/compare_normal_forecasts_filtered} 
  \end{center}
  \caption{Comparing filtered 5-minute variance forecasts with 5-minute variance forecasts. The filtration dampned the effects of the unusual 5-minute returns in July.}\label{fig}
\end{figure*}

```{r, results = 'asis', echo = FALSE, cache = TRUE}
all_table_data <- readRDS("../../paper-tables/performance_table_data_filtered.rds") 

mse_mape <- all_table_data %>%
  filter(distribution == "Normal") %>%
  select(-distribution) %>%
  gather(key = "proxy", value = "variance_proxy", -date, -daily_variance_forecast, -forc_style) %>%
  group_by(forc_style, proxy) %>%
  summarise(mse  = mean((variance_proxy - daily_variance_forecast) ^ 2),
            rmse = sqrt(mse),
            mape = mean(abs(variance_proxy - daily_variance_forecast) / sqrt(variance_proxy))) %>%
  gather(key = "metric", value = "value", -forc_style, -proxy) %>%
  spread(forc_style, value) %>%
  filter(metric != "mse")

MZ <- all_table_data %>%
  filter(distribution == "Normal") %>%
  select(-distribution) %>%
  gather(key = "proxy", value = "variance_proxy", -date, -daily_variance_forecast, -forc_style) %>%
  group_by(forc_style, proxy) %>%
  nest() %>%
  mutate(lm_fit = map(data, ~lm(variance_proxy ~ daily_variance_forecast, data = .x)),
         tidy = map(lm_fit, broom::tidy),
         glance = map(lm_fit, broom::glance)) 

MZ_coef <- MZ %>%
  unnest(tidy, .drop = TRUE) %>%
  select(forc_style:std.error) %>%
  gather(key, value, -forc_style, -proxy, -term) %>%
  unite(col = "metric", term, key) %>%
  spread(forc_style, value)

MZ_r2 <- MZ %>%
  unnest(glance, .drop = TRUE) %>%
  select(forc_style:r.squared) %>%
  gather(metric, value, r.squared) %>%
  spread(forc_style, value)

hux_data <- bind_rows(mse_mape, MZ_coef, MZ_r2) %>%
  mutate(metric = recode(metric, "mape" = "MAPE", "rmse" = "RMSE", 
                         "(Intercept)_estimate" = "MZ Intercept", 
                         "(Intercept)_std.error" = "MZ Intercept Std. Error",
                         "daily_variance_forecast_estimate" = "MZ Slope",
                         "daily_variance_forecast_std.error" = "MZ Slope Std. Error",
                         "r.squared" = "MZ $R^2$")) %>%
  arrange(proxy) %>%
  select(metric, "5-Min Filtered" = "5min_filtered")

  
triple_blank <- function() {
  c("", "", "")
}

hux_data <- hux_data %>%
  mutate_if(is.numeric, round, 5)

hux_data[with(hux_data, which(metric == "MZ Intercept Std. Error" | metric == "MZ Slope Std. Error")),] <- mutate_if(hux_data[with(hux_data, which(metric == "MZ Intercept Std. Error" | metric == "MZ Slope Std. Error")),],
                                                                                   is.double, 
                                                                                   funs(paste0("(", ., ")")))

hux_data[with(hux_data, which(metric == "MZ Intercept Std. Error" | metric == "MZ Slope Std. Error")),] <- mutate(hux_data[with(hux_data, which(metric == "MZ Intercept Std. Error" | metric == "MZ Slope Std. Error")),],
                                                                                metric = replace(metric, 1:4, NA))

ht <- hux_data %>%
  huxtable() %>%
  
  # Column names
  add_colnames() %>%
  
  # Proxy rows
  insert_row("Proxy: RV",    "", after = 1) %>%
  insert_row("Proxy: $r^2$", "", after = 9) %>%
  
  # Number rounding
  set_number_format(everywhere, everywhere, "%5.4f") %>%
  
  # Bold
  set_bold(matrix(c(2,10)), 1, TRUE) %>%
  set_bold(1, everywhere, TRUE) %>%
  
  # Alignment
  set_align(everywhere, everywhere, 'center') %>%
  set_align(everywhere, 1, 'right') %>%
  
  # Padding
  set_all_padding(value = 10) %>%
  set_top_padding(c(6, 8, 14, 16), everywhere, 0) %>%
  set_bottom_padding(c(5, 7, 13, 15), everywhere, 0) %>%

  # Borders
  set_bottom_border(matrix(c(1, 9)), everywhere, value = .3) %>%
  set_right_border(everywhere, 1, .3) %>%
  
  # Escape latex
  set_escape_contents(everywhere, 1, FALSE) %>%
  set_escape_contents(1, everywhere, FALSE) %>%
  
  set_caption("Forecast performance of the filtered 5-minute model. Performance metrics are on par with the results from removing the extreme outlier from the non-filtered model.")


ht[1,1] <- ""

ht
```


# Conclusion

This paper analyzed the daily volatility of Bitcoin returns using GARCH modeling.
Of note, two forecasting techniques were used, a traditional forecasting method
where the model is fit to daily data to predict daily data, and a more granular
model fit to 5-minute data where daily forecasts are generated from aggregating
multi-step ahead 5-minute forecasts. The performance was measured against two
proxies, the common, but noisy, daily squared return, and realized 
variance.

Forecasts generated from the 5-minute method were overall more accurate than traditional
GARCH modeling forecasts, but care must be taken to deal with outliers. Models
fit under the Normal distribution do not produce forecasts that are practically
different from Student-t distributions. The realized variance proxy is a much more
promising estimate of daily variance than the squared daily return. Looking only at
the MZ regression $R^2$ values demonstrates this, where forecasts compared against
daily squared returns present $R^2$ values near 0.

Future research could focus on creating models that better handled 
the large swings in short term volatility so that a filtration would not have
to be applied. Additionally, other modeling periodicities could be tried to find
an optimal forecasting granularity.

